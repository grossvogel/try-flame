# TryFlame

This is just a little toy project to test out the FLAME library for Elixir. You type some text, and the app generates an emoji response to that text, sharing with anybody who's online at that time.

The cool thing is that the language model that comes up with the emoji response is running in on-demand compute instances on Fly.io. They spin up on the first request, hang out and process emoji requests as needed, and spin themselves down after 30 seconds of idleness. They also run a different (beefier) machine spec than the one used for the web app. They don't use GPU at this time, but they could take advantage of that if I took the time to augment the Docker image with the required libs.

Until I tear it down, you can find it at https://try-flame.fly.dev/

## Implementation
Interesting parts of the code that will help understand what's involved in integrating FLAME:

- [mix.exs](./mix.exs) - of course we have to add the dependency (as well as Bumblebee and EXLA for the machine learning stuff)
- [application.ex](./lib/try_flame/application.ex) is where we configure the supervision tree for the app, so there are some key bits here. The important thing to know is that we now have to start different sets of children depending on whether we're running the main app or a FLAME worker.
  - Add a `FLAME.Pool` child, along with some configuration including its name (presumably you can have multiple for different types of workloads), concurrency and timeout settings.
  - If we are starting a FLAME worker (or running locally), we start a `Nx.Serving` process defined in [classification.ex](./lib/try_flame/classification.ex). This is where the machine learning happens, but if we're only starting the bare web app, we skip it and avoid the beefy RAM requirements.
  - If we aren't starting a FLAME worker, we start the Phoenix Endpoint, aka the web server.
- [classification.ex](./lib/try_flame/classification.ex) - This file probably has more disparate concerns in it than I'd do in a real app... it has essentiall _all_ of the biz logic in it:
  - Define the `Nx.Serving` to generate emoji responses, with a bare synchronous `emojify` function that invokes the serving with a particular text input.
  - The `async_emojify` function that casts to a FLAME worker, invoking the `emojify` function on the worker and broadcasting the result over PubSub. Note that _nearly all_ of the code in this function is around timing the execution and formatting the PubSub payload.
  - A separate function to load the serving from a release... this is used in the Dockerfile so we can bake the model and params into the [Dockerfile](./Dockerfile) rather than downloading them on demand for each worker.
- [the main LiveView](./lib/try_flame_web/live/classification/index.ex) isn't anything special. The most interesting part is that it calls the `async_emojify` function _inside a task_ because the actual `FLAME.cast()` call doesn't return quickly enough if it has to spin up a new worker. It also subscribes to PubSub to be aware whenever a new Emoji has been generated by a FLAME worker.

### FLAME usage
There are a couple of ways to invoke FLAME:
- `cast` is what we're doing here, which (just like with a GenServer) doesn't wait for a reply. It's fire-and-forget for the client, and in our case the LiveView finds out when the new Emoji is ready via the PubSub message.
- `call` is blocking. While the FLAME worker will actually do the work, the caller will pause execution until the work is done and a response is returned.
- `place_child` lets you start up a process like a GenServer on a FLAME worker, instead of just starting a single function.

### FLAME backends
Because this is deployed in Fly.io. we're using the Fly backend that's built into FLAME, as configured in [runtime.exs](./config/runtime.exs). The default backend just runs things on the same node, so it "just works" when you run it locally w/o that config.

The [FLAME.Behaviour](https://hexdocs.pm/flame/FLAME.Backend.html) defines what a backend needs to do, and it's pretty simple. Four required callbacks and one optional one. The [FlyBackend](https://github.com/phoenixframework/flame/blob/main/lib/flame/fly_backend.ex) is only a couple hundred lines of code, though it's definitely not trivial.

There's also a backend for Kubernetes, but I haven't had a chance to look at it much yet: https://github.com/mruoss/flame_k8s_backend


## Interesting Concepts
In the Thinking Elixir episode, Chris takes pains to point out some interesting distinctions between concepts that we sometimes tempt to bundle together.

- **Concurrency** is something we like for many tasks, because it allows us to do multiple things at once, with some in the background while the "main" process keeps going. Elixir's Task (supervised or not) gives us concurrency (see also Promises in JS).
- **Durability** is something we like as well. Sometimes we want a task to be retried if it fails, and we want a permanent (or somewhat long-lived) record of what happened, especially if it fails. We typically use Oban for this. We get concurrency from Oban as well, but if we don't care about durability then we might as well just use a Task.
- **Elastic Compute** is primarily what FLAME gives us that we couldn't get before. We could choose to run an Oban job that calls out to FLAME if we want to offload that compute, and we could probably use `FLAME.call` in that case b/c we don't care how long the Oban job takes and it probably doesn't have anything else to do in the meantime. FLAME also makes it easier to run heterogenous workloads on more appropriate hardware w/o managing multiple app deployments.


## References
- [Documentation](https://hexdocs.pm/flame/FLAME.html)
- [Blog Post](https://fly.io/blog/rethinking-serverless-with-flame/)
- [YouTube Video](https://www.youtube.com/watch?v=l1xt_rkWdic)
- [Thinking Elixir Podcast](https://podcast.thinkingelixir.com/181)
- [Hacker News Discussion](https://news.ycombinator.com/item?id=38542764)

